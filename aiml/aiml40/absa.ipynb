{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License.\n\n### Intel NLP-Architect ABSA on AzureML \n\nThis notebook contains an end-to-end walkthrough of using Azure Machine Learning Service to train, finetune and test [Aspect Based Sentiment Analysis Models using Intel's NLP Architect](http://nlp_architect.nervanasys.com/absa.html)\n\n### Prerequisites\n\n* Understand the architecture and terms introduced by Azure Machine Learning (AML)\n* Have working Jupyter Notebook Environment. You can:\n    - Install Python environment locally, as described below in **Local Installation**\n    - Use [Azure Notebooks](https://docs.microsoft.com/ru-ru/azure/notebooks/azure-notebooks-overview/?wt.mc_id=absa-notebook-abornst). In this case you should upload the `absa.ipynb` file to a new Azure Notebooks project, or just clone the [GitHub Repo](https://github.com/microsoft/ignite-learning-paths/tree/master/aiml/aiml40).\n* Azure Machine Learning Workspace in your Azure Subscription\n\n#### Local Installation\n\nInstall the Python SDK: make sure to install notebook, and contrib:\n\n```shell\nconda create -n azureml -y Python=3.6\nsource activate azureml\npip install --upgrade azureml-sdk[notebooks,contrib] \nconda install ipywidgets\njupyter nbextension install --py --user azureml.widgets\njupyter nbextension enable azureml.widgets --user --py\n```\n\nYou will need to restart jupyter after this Detailed instructions are [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python/?WT.mc_id=absa-notebook-abornst)\n\nIf you need a free trial account to get started you can get one [here](https://azure.microsoft.com/en-us/offers/ms-azr-0044p/?WT.mc_id=absa-notebook-abornst)\n\n#### Creating Azure ML Workspace\n\nAzure ML Workspace can be created by using one of the following ways:\n* Manually through [Azure Portal](http://portal.azure.com/?WT.mc_id=absa-notebook-abornst) - [here is the complete walkthrough](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace/?wt.mc_id=absa-notebook-abornst)\n* Using [Azure CLI](https://docs.microsoft.com/ru-ru/cli/azure/?view=azure-cli-latest&wt.mc_id=absa-notebook-abornst), using the following commands:\n\n```shell\naz extension add -n azure-cli-ml\naz group create -n absa -l westus2\naz ml workspace create -w absa_space -g absa\n```"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Initialize workspace\n\nTo access an Azure ML Workspace, you will need to import the AML library and the following information:\n* A name for your workspace (in our example - `absa_space`)\n* Your subscription id (can be obtained by running `az account list`)\n* The resource group name (in our case `absa`)\n\nInitialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace/?WT.mc_id=absa-notebook-abornst) object from the existing workspace you created in the Prerequisites step or create a new one. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n#subscription_id = ''\n#resource_group  = 'absa'\n#workspace_name  = 'absa_space'\n#ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n#ws.write_config()\n\ntry:\n    ws = Workspace.from_config()\n    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n    print('Library configuration succeeded')\nexcept:\n    print('Workspace not found')",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "abla_space\twesteurope\tabla\twesteurope\nLibrary configuration succeeded\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Compute"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are two computer option run once(preview) and persistent compute for this demo we will use persistent compute to learn more about run once compute check out the [docs](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute?WT.mc_id=absa-notebook-abornst)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\n# Choose a name for your CPU cluster\ncluster_name = \"absa-cluster\"\n\n# Verify that cluster does not exist already\ntry:\n    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n    print('Found existing cluster, use it.')\nexcept ComputeTargetException:\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n                                                           vm_priority='lowpriority',\n                                                           min_nodes=1,\n                                                           max_nodes=1)\n    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n\ncluster.wait_for_completion(show_output=True)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found existing cluster, use it.\nSucceeded\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Upload Data\n\nThe dataset we are using comes from trip advisor and is in the open domain, this can be replaced with any csv file with rows of text as the absa model is unsupervised. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!wget https://raw.githubusercontent.com/NervanaSystems/nlp-architect/master/datasets/absa/tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "--2019-09-20 10:26:47--  https://raw.githubusercontent.com/NervanaSystems/nlp-architect/master/datasets/absa/tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv\nResolving webproxy (webproxy)... 10.36.6.1\nConnecting to webproxy (webproxy)|10.36.6.1|:3128... connected.\nProxy request sent, awaiting response... 200 OK\nLength: 961388 (939K) [text/plain]\nSaving to: ‘tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv’\n\ntripadvisor_co_uk-t 100%[===================>] 938.86K  2.39MB/s    in 0.4s    \n\n2019-09-20 10:26:48 (2.39 MB/s) - ‘tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv’ saved [961388/961388]\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os                            \nlib_root = os.path.dirname(os.path.abspath(\"__file__\"))\nds = ws.get_default_datastore()\nds. upload_files([os.path.join(lib_root,'tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv')], \n                relative_root=lib_root)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train File"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile train.py\nimport argparse\nimport os \nfrom azureml.core import Run\nfrom spacy.cli.download import download as spacy_download\nfrom nlp_architect.models.absa.train.train import TrainSentiment\nfrom nlp_architect.models.absa import TRAIN_OUT\nfrom nlp_architect.utils.io import download_unzip\n\nspacy_download('en')\nEMBEDDING_URL = 'http://nlp.stanford.edu/data', 'glove.840B.300d.zip'\nEMBEDDING_PATH = TRAIN_OUT / 'word_emb_unzipped' / 'glove.840B.300d.txt'\ndownload_unzip(*EMBEDDING_URL, EMBEDDING_PATH)\n\nparser = argparse.ArgumentParser(description='ABSA Train')\nparser.add_argument('--data_folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--learning_rate', type=float, default=3e-5, help='learning rate')\nparser.add_argument('--epochs', type=int, default=5)\nargs = parser.parse_args()\n\n\nrerank_model = None # Path to rerank model .h5 file\nparsed_data = None\n\ntripadvisor_train = os.path.join(args.data_folder, \n                                 'tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv')\n\nos.makedirs('outputs', exist_ok=True)\n    \n\ntrain = TrainSentiment(parse=not parsed_data, rerank_model=rerank_model)\n\n\nopinion_lex, aspect_lex = train.run(data=tripadvisor_train,\n                                    out_dir = './outputs',\n                                    parsed_data=parsed_data)\n\n# get hold of the current run\nrun = Run.get_context()\n\nrun.log('Aspect Lexicon Size:', len(aspect_lex))\nrun.log('Opinion Lexicon Size:', len(opinion_lex))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting train.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create An Expierment\n\nCreate an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment/?WT.mc_id=absa-notebook-abornst) to track all the runs in your workspace for this distributed PyTorch tutorial. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\nexperiment_name = 'absa'\n\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data_folder': ds,\n}\n\n# find a way to integrate nlp architect \nnlp_est = Estimator(source_directory='.',\n                   script_params=script_params,\n                   compute_target=cluster,\n                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n                   entry_script='train.py',\n                   pip_packages=['git+https://github.com/NervanaSystems/nlp-architect.git@absa'])\n",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = exp.submit(nlp_est)\nrun.id\nrun_id = run.id",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "'absa_1568985331_df076c3c'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = [r for r in exp.get_runs() if r.id == 'absa_1568985331_df076c3c'][0]",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\nRunDetails(run).show()",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e52608b49f274a5a804b95af28165038",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fine-Tuning NLP Archictect  with AzureML HyperDrive\nAlthough ABSA is an unsupervised method it can be fined tuned if provided with a small sample of labeled data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import *\nimport math\n\nparam_sampling = RandomParameterSampling( {\n        'asp_thresh': list(range(1,5)),\n         'op_thresh': 2, \n         'max_iter': list(range(1,5))\n    }\n)\n\nhyperdrive_run_config = HyperDriveRunConfig(estimator=nlp_est,\n                                            hyperparameter_sampling=param_sampling, \n                                            primary_metric_name='f1', # This requires a modification of script to finetune on supervised data\n                                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                            max_total_runs=16,\n                                            max_concurrent_runs=4)",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "HyperDriveRunConfig is deprecated. Please use the new HyperDriveConfig class.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, lauch the hyperparameter tuning job."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment = Experiment(workspace=ws, name='hyperdrive')\nhyperdrive_run = experiment.submit(hyperdrive_run_config)",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Monitor HyperDrive runs\nWe can monitor the progress of the runs with the following Jupyter widget. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\nRunDetails(hyperdrive_run).show()",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "675d725b1e64421c94d55fe640f35f84",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1046ca3945a947a28601ebcf88b0e153",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Find and register the best model\nOnce all the runs complete, we can find the run that produced the model with the highest evaluation (METRIC TBD)."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "best_run = hyperdrive_run.get_best_run_by_primary_metric()\nbest_run_metrics = best_run.get_metrics()\nprint(best_run)\nprint('Best Run is:\\n  F1: {0:.5f} \\n  Learning rate: {1:.8f}'.format(\n        best_run_metrics['eval_f1'][-1],\n        best_run_metrics['lr']\n     ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Register Model Outputs"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "aspect_lex = run.register_model(model_name='aspect_lex', model_path='outputs/train_out/generated_aspect_lex.csv')\nopinion_lex = run.register_model(model_name='opinion_lex', model_path='outputs/train_out/generated_opinion_lex_reranked.csv')",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Deploy as web service\nOnce you've tested the model and are satisfied with the results, deploy the model as a web service hosted in [Azure Container Instances](https://azure.microsoft.com/en-us/services/container-instances/?WT.mc_id=bert-notebook-abornst).\n\nTo build the correct environment for ACI, provide the following:\n\nA scoring script to show how to use the model\nAn environment file to show what packages need to be installed\nA configuration file to build the ACI\nThe model you trained before\n\n## Create scoring script\nCreate the scoring script, called score.py, used by the web service call to show how to use the model.\n\nYou must include two required functions into the scoring script:\n\nThe init() function, which typically loads the model into a global object. This function is run only once when the Docker container is started.\n\nThe run(input_data) function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%writefile score.py\nfrom azureml.core.model import Model\nfrom nlp_architect.models.absa.inference.inference import SentimentInference\nfrom spacy.cli.download import download as spacy_download\n\n\ndef init():\n    \"\"\"\n    Set up the ABSA model for Inference  \n    \"\"\"\n    global inference\n    spacy_download('en')\n    aspect_lex = Model.get_model_path('aspect_lex')\n    opinion_lex = Model.get_model_path('opinion_lex')    \n    inference = SentimentInference(aspect_lex, opinion_lex)\n\ndef run(raw_data):\n    \"\"\"\n    Evaluate the model and return JSON string\n    \"\"\"\n    sentiment_doc = inference.run(doc=raw_data)\n    return sentiment_doc.json()",
      "execution_count": 113,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Overwriting score.py\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create configuration files\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ACI Config\nCreate a ACI configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service.`"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1,  \n                                               tags={\"data\": \"text\",  \"method\" : \"NLP Architcet ABSA\"}, \n                                               description='Predict ABSA with NLP Architect')",
      "execution_count": 99,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create Enviorment File\ncreate an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs nlp-architect and the azureml-sdk. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\npip = [\"azureml-defaults\", \"azureml-monitoring\", \"git+https://github.com/NervanaSystems/nlp-architect.git@absa\"]\n\nmyenv = CondaDependencies.create(pip_packages=pip)\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())",
      "execution_count": 100,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create Environment Config\nCreate a Enviorment configuration file and specify the enviroment and enviormental variables required for the application"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core import Environment\ndeploy_env = Environment.from_conda_specification('absa_env', \"myenv.yml\")\ndeploy_env.environment_variables={'NLP_ARCHITECT_BE': 'CPU'}",
      "execution_count": 101,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Inference Config \nCreate an inference configuration that recieves the deployment enviorment and the entry script"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(environment=deploy_env,\n                                   entry_script=\"score.py\")",
      "execution_count": 102,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Deploy in ACI\nEstimated time to complete: about 7-8 minutes\n\nConfigure the image and deploy. The following code goes through these steps:\n\nBuild an image using:\nThe scoring file (score.py)\nThe environment file (myenv.yml)\nThe model file\nRegister that image under the workspace.\nSend the image to the ACI container.\nStart up a container in ACI using the image.\nGet the web service HTTP endpoint.\nhttps://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-azure-container-instance"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\n\nfrom azureml.core.webservice import AciWebservice, Webservice\nfrom azureml.core.model import Model\n\naspect_lex = Model(ws, 'aspect_lex')\nopinion_lex = Model(ws, 'opinion_lex')    \n\nservice = Model.deploy(workspace=ws,\n                       name='absa-srvc', \n                       models=[aspect_lex, opinion_lex],\n                       inference_config=inference_config, \n                       deployment_config=aciconfig)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)",
      "execution_count": 114,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Creating service\nRunning.........................................\nSucceededACI service creation operation finished, operation \"Succeeded\"\nHealthy\nCPU times: user 397 ms, sys: 69.6 ms, total: 466 ms\nWall time: 3min 45s\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "View service logs: This is powerful for debugging"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(service.get_logs())\n",
      "execution_count": 115,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n2019-08-26T13:07:50,262247639+00:00 - gunicorn/run \n2019-08-26T13:07:50,262797544+00:00 - iot-server/run \n2019-08-26T13:07:50,262247539+00:00 - rsyslog/run \n2019-08-26T13:07:50,263256549+00:00 - nginx/run \nbash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by bash)\n/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\nStarting gunicorn 19.9.0\nListening at: http://127.0.0.1:31311 (12)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 37\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n2019-08-26T13:07:55,845199922+00:00 - iot-server/finish 1 0\n2019-08-26T13:07:55,846288933+00:00 - Exit code 1 is normal. Not restarting iot-server.\nInitializing logger\nStarting up app insights client\nStarting up request id generator\nStarting up app insight hooks\nInvoking user's init function\nCollecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py): started\n  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=57529de5476488be86a3aacd1d1228af49ffcfd106e71516c94f39750dbd060a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-dylq_y2m/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.1.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n\u001b[38;5;2m✔ Linking successful\u001b[0m\n/azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/python3.6/site-packages/en_core_web_sm\n-->\n/azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/python3.6/site-packages/spacy/data/en\nYou can now load the model via spacy.load('en')\n2019-08-26 13:08:04,646 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}, switching offline: False\n2019-08-26 13:08:04,647 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | version is None. Latest version is 2\n2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | Found model path at azureml-models/aspect_lex/2/generated_aspect_lex.csv\n2019-08-26 13:08:04,647 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}, switching offline: False\n2019-08-26 13:08:04,647 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n2019-08-26 13:08:04,648 | azureml.core.model | DEBUG | version is None. Latest version is 1\n2019-08-26 13:08:04,648 | azureml.core.model | DEBUG | Found model path at azureml-models/opinion_lex/1/generated_opinion_lex_reranked.csv\nUsing pre-trained BIST model.\nDownloading pre-trained BIST model...\nDownloading file to: /root/nlp-architect/cache/bist-pretrained/bist-pretrained.zip\n  0%|          | 0/24 [00:00<?, ?MB/s]\n  4%|▍         | 1/24 [00:00<00:08,  2.57MB/s]\n 12%|█▎        | 3/24 [00:00<00:06,  3.38MB/s]\n 33%|███▎      | 8/24 [00:00<00:03,  4.63MB/s]\n 58%|█████▊    | 14/24 [00:00<00:01,  6.28MB/s]\n 83%|████████▎ | 20/24 [00:01<00:00,  8.38MB/s]\n25MB [00:01, 22.74MB/s]\n[dynet] random seed: 2734000164\n[dynet] allocating memory: 512MB\n[dynet] memory allocation done.\nDownload Complete\nUnzipping...\nDone.\nUsers's init has completed successfully\nScoring timeout setting is not found. Use default timeout: 3600000 ms\n\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "service = ws.webservices['absa-srvc']",
      "execution_count": 116,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application.\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(service.scoring_uri)",
      "execution_count": 117,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "http://401d4329-3c4c-4187-97ec-3cad2d439708.eastus.azurecontainer.io/score\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test Deployed ACI Service"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import requests\nimport json\nfrom nlp_architect.models.absa.inference.data_types import TermType\n\n# send a random row from the test set to score\ninput_data = \"The ambiance is charming. Uncharacteristically, the service was DREADFUL.\\\n              When we wanted to pay our bill at the end of the evening, our waitress was nowhere to be found...\"\n\nheaders = {'Content-Type':'application/json'}\n\nresp = requests.post(service.scoring_uri, input_data, headers=headers)\nresp.json()",
      "execution_count": 121,
      "outputs": [
        {
          "data": {
            "text/plain": "'{\"_doc_text\": \"The ambiance is charming. Uncharacteristically, the service was DREADFUL.              When we wanted to pay our bill at the end of the evening, our waitress was nowhere to be found...\", \"_sentences\": [{\"_start\": 0, \"_end\": 24, \"_events\": [[{\"_text\": \"ambiance\", \"_type\": \"ASPECT\", \"_polarity\": \"POS\", \"_score\": 1.0, \"_start\": 4, \"_len\": 8}, {\"_text\": \"charming\", \"_type\": \"OPINION\", \"_polarity\": \"POS\", \"_score\": 1.0, \"_start\": 16, \"_len\": 8}]]}, {\"_start\": 26, \"_end\": 72, \"_events\": [[{\"_text\": \"service\", \"_type\": \"ASPECT\", \"_polarity\": \"NEG\", \"_score\": -1.0, \"_start\": 52, \"_len\": 7}, {\"_text\": \"DREADFUL\", \"_type\": \"OPINION\", \"_polarity\": \"NEG\", \"_score\": -1.0, \"_start\": 64, \"_len\": 8}]]}, {\"_start\": 87, \"_end\": 183, \"_events\": [[{\"_text\": \"waitress\", \"_type\": \"ASPECT\", \"_polarity\": \"NEG\", \"_score\": -0.98065746, \"_start\": 149, \"_len\": 8}, {\"_text\": \"waitress\", \"_type\": \"OPINION\", \"_polarity\": \"NEG\", \"_score\": -0.98065746, \"_start\": 149, \"_len\": 8}]]}]}'"
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Render the response using [Displacy](https://spacy.io/usage/visualizers/)\nNote ```Spacy``` Must be installed on the local machine for this to work can be installed with ```pip install spacy```"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import spacy\nfrom spacy import displacy\n\nif resp.text:\n    doc = json.loads(resp.json()) # load response as dictionary\n    doc_viz = {'text':doc[\"_doc_text\"], 'ents':[]}\n    for s in doc[\"_sentences\"]:\n        for e in s[\"_events\"][0]:\n            if e[\"_type\"] == \"ASPECT\":\n                doc_viz['ents'].append({'start': e[\"_start\"], 'end': e[\"_start\"] + e[\"_len\"], 'label':str(e[\"_polarity\"])})\n    doc_viz['ents'].sort(key=lambda m: m[\"start\"])\n    displacy.render(doc_viz, style=\"ent\", options={'colors':{'POS':'#7CFC00', 'NEG':'#FF0000'}}, manual=True)",
      "execution_count": 122,
      "outputs": [
        {
          "data": {
            "text/html": "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n<mark class=\"entity\" style=\"background: #7CFC00; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    ambiance\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">POS</span>\n</mark>\n is charming. Uncharacteristically, the \n<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    service\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NEG</span>\n</mark>\n was DREADFUL.              When we wanted to pay our bill at the end of the evening, our \n<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    waitress\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NEG</span>\n</mark>\n was nowhere to be found...</div>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}